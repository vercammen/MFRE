--- 
title: "Time Series Analysis of Food and Energy Prices"
author: "James Vercammen"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This chapter describes the data preparation procedures.
link-citations: yes
github-repo: rstudio/bookdown-demo
---

# Data Preparation

This chapter shows the R programming steps which are used to prepare the data for the various empirical applications in this textbook. The textbook has eight chapters with empirical applications and each chapter has one or more applications. For each application the raw data is read into R from an API such as the Federal Reserve Economic Data (FRED) or from an Excel or csv file. The prepared data for each application is saved into an *.RDS* file. The collection of *.RDS* files for a particular chapter are read into R when the code for that chapter is run. Another word for data preparation is data wrangling. Part of the data preparation process may involve data cleaning, which is typically used to deal with missing data and mistakes in the data.
 
Section \@ref(sec:intro) provides a brief introduction, which includes a link to a blog post about data preparation best practices and the links to the various websites which contain the raw data to be prepared. In Section \@ref(sec:packages) the general data import procedures are described and the various R packages which are used for the data preparation are loaded. The data preparation procedures for each of the empirical applications in Chapters 1 and 4 through 11 are described in Section \@ref(sec:applications). Section \@ref(sec:rds) contains the optional code for saving the prepared data sets to individual *.RDS* files.

## Introduction {#sec:intro}

Time series raw data must typically be prepared before it can be analyzed. In some cases simple steps such as filtering for the correct range of dates and identifying the date column for creating the time series variable is all that is is needed. In other cases, considerable data wrangling is required. This includes omitting rows of a csv or Excel file when importing, converting column names into strings which are compatible with R (e.g., no spaces) and converting data which arrives in character format into date format.    

Coding practices vary across individuals but it is nevertheless important to adhere to coding best practices when preparing data. A good summary of the general coding practices which are relevant for R can be found at <https://style.tidyverse.org/syntax.html>.

The specific data preparation procedures are shown for the applications in the following chapters. Unless otherwise noted, the variables refer to prices for U.S. commodities, measured in U.S. dollars.  

* Chapter 1: Global wheat and crude oil.
* Chapter 4: Bananas, consumer price index (CPI), Crude oil (daily) and potatoes.
* Chapter 5: CPI, industrial production index, wheat and crude oil, Alberta eggs.
* Chapter 6: Gasoline and electricity.
* Chapter 7: Fruit, Vancouver gasoline, corn, energy consumption temperature anomalies and hog prices.
* Chapter 8: Wood chips and heating oil prices.
* Chapter 9: Vegetable oil prices and biofuel prices.
* Chapter 10: Canada and U.S. crude oil prices, and Veggie CPI, drought and diesel fuel prices.
* Chapter 11: To be determined. 

The on-line links to these data sets are as follows:

| Chpt | Data  | URL |
|:---|:-----|:------|
| 1 |  Global wheat  | <https://fred.stlouisfed.org/series/PWHEAMTUSDM>  |
| 1 |  WTI crude oil (monthly)  | <https://fred.stlouisfed.org/series/MCOILWTICO>  |
| 4 |  Bananas  |<https://www.indexmundi.com/commodities/?commodity=bananas&months=240> |
| 4 |  WTI crude oil (monthly) | Same as Chapter 1.
| 4 |  WTI crude (daily-web API) | <https://eodhistoricaldata.com>  |
| 4 |  Consumer price index | <https://fred.stlouisfed.org/series/CPIAUCSL>  |
| 4 |  Farm price of potatoes | <https://fred.stlouisfed.org/series/WPU01130603>  |
| 5 |  Industrial production index | <https://fred.stlouisfed.org/series/IPUTIL>  |
| 5 |  Wheat and oil | Same as Chapter 1. |
| 5 |  Eggs (Alberta) | <https://agriculture.canada.ca/en/market-information-system/rp/index-eng.cfm?action=pR&r=16&pdctc=&wbdisable=true> |
| 6 |  Gasoline (Vancouver) | <https://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=1810000101> |
| 6 |  Electricity price Index  | <https://fred.stlouisfed.org/series/CUUR0000SEHF01> |
| 6 |  Natural gas  | <https://fred.stlouisfed.org/series/MHHNGSP> |
| 7 |  Strawberries | <https://fred.stlouisfed.org/series/APU0000711415> |
| 7 |  Grapes | <https://fred.stlouisfed.org/series/APU0000711417> |
| 7 |  U.S. Mexico exchange | <https://fred.stlouisfed.org/series/EXMXUS> |
| 7 |  Seasonal adjusted gasoline|<https://fred.stlouisfed.org/series/CUSR0000SETB01>|
| 7 |  Non SA gasoline|<https://fred.stlouisfed.org/series/CUUR0000SETB01>|
| 7 | Corn farm price|<https://www.ers.usda.gov/data-products/feed-grains-database/>|
| 7 |  Energy consumption|<https://www.eia.gov/totalenergy/data/monthly/>|
| 7 |  Temperature anomaly|<https://www.ncei.noaa.gov/access/monitoring/national-temperature-index/time-series/anom-tavg/1/12>|
| 7 |  Hog PPI|<https://fred.stlouisfed.org/series/WPU0132>|
| 7 |  U.S. Canada exchange|<https://fred.stlouisfed.org/series/EXCAUS>|
| 8 |  Wood chips|<https://fred.stlouisfed.org/series/PCU3211133211135>|
| 8 |  Plywood |<https://fred.stlouisfed.org/series/WPU083>|
| 8 |  Heating Oil |<https://fred.stlouisfed.org/series/WPU05730201>|
| 9 |  Vegetable oils |<https://www.worldbank.org/en/research/commodity-markets>|
| 9 |  Biodiesel, soyoil |<https://www.card.iastate.edu/research/biorenewables/tools/hist_bio_gm.aspx>| 
| 9 |  Diesel|<http://www.eia.gov/oog/info/wohdp/diesel.asp>|
| 9|Weekly WTI|<https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=PET&s=RWTC&f=W>|
| 10 |  Canada-U.S. crude |<https://economicdashboard.alberta.ca/oilprice>|
| 10 |  U.S. dollar index |<https://fred.stlouisfed.org/series/TWEXBGSMTH>|
| 10 | Diesel |<https://fred.stlouisfed.org/series/GASDESW>|
| 10 |  Veggie CPI |<https://fred.stlouisfed.org/series/CUSR0000SAF113>|
| 10 | California drought| <https://droughtmonitor.unl.edu/DmData/DataTables.aspx?state,ca>  |
 
In addition to the prepared data sets, Chapters 6 and 7 each use simulated data. The code used to generate the simulated data are bundled with the corresponding chapter's data preparation code.

## Data Import and Packages {#sec:packages}

There are three common ways to import data into R: (1) API package method; (2) API using URL address; and (3) read data from a locally-stored Excel or csv file. We will use all three methods in this chapter.  

The widely used *tidyverse* package includes *readxl* and *readr*. Note that *readxl* is not part of the core *tidyverse* package and so it must be loaded using "library(readxl)" even if the *tidyverse* package has been loaded. In contrast, *readr* is part of the *tidyverse* core and so it is not required to use "library(readr)" if the *tidyverse* package is loaded. 

For some of the applications we will use the *read_excel()* function from the *readxl* package to read in data from an Excel file. We will use the *read_csv()* function from the *readr* package when reading from a csv file. If the date is in text format with a tab separation or some other data format then the *read_tsv()* or *read_delim()* functions should be used.   

When the data is read in from Excel or csv there is no need to include in the read command a statement which specifies how the data is organized (e.g., sep = ','). 

In most cases the top row of the data contains the names of the columns (i.e., the header). If your data does not have a header then you need to use "header = FALSE" in your Excel or csv read statement.

In the older versions of R, it was important to use "stringsAsFactors=FALSE" in the Excel or csv read statement. With current versions of R this restriction is now the default setting and so it is no longer necessary to include this statement when importing data.

If you believe there are white spaces in some of the data (e.g., "re d" instead of "red") then the data can be read in from Excel or a csv file with "strip.white = TRUE" included in the read statement.

When reading in data the default is for R to assign to "1, 2, 3..." as row names. If you do not wish row names then use "row.names = FALSE" in the read statement.

The four data preparation modules below have a similar format.

* Import data with an API or read data from Excel or csv into a tibble object.
* Convert date columns from character format to R date format if required.
* Select columns and dates to retain.
* Convert the tibble object to a time series tsibble object.
* Merge data sets if required.

The data preparation concludes by saving the final tsibble object as a .RDS file. These .RDS files will be imported and used in the empirical applications in the chapters to follow.

The preparation of the four data sets requires a number of packages to be loaded. Packages are typically loaded with the *library()* function. Rather than specifying a separate library function for each package to be loaded, the *p_load()* function from R's *pacman* package can be used instead. Specifically,

*pacman::p_load(fpp3, tidyverse, readxl, lubridate, here, janitor, xts, tsibble, forecast, fredr,reshape2)*

Loading the *tidyverse* package automatically loads the core set of related packages, which include: *ggplot2*, *dplyr*, *tidyr*, *readr*, *purrr*, *tibble*, *stringr* and *forcats*. It was previously noted that although *readxl* is part of the *tidyverse* family it is not automatically loaded when *tidyverse* is loaded. The situation is the same for the *lubridate* package. The *here* package is used as part of the R project configuration. The *janitor* package allows the *clean_names()* function to be used. The remaining packages, *xts*, *urca*, *tsibble* and *forecast* are used as part of the time series analysis.

The code for loading the packages for this chapter is as follows:

```{r packages}
pacman::p_load(fpp3, tidyverse, readxl, lubridate, here, janitor, tsibble, xts, forecast, fredr,fastDummies,fable,reshape2)

```

## Applications {#sec:applications}

In this section the data preparation procedures are described for the various empirical application chapters in this textbook (Chapter 1, and Chapters 4 through 11). This section begins with a brief discussion about using APIs to access data.

### Accessing Data with an API

There are many websites which offer subscription-based data APIs for the retrieval of various types of data including commodity prices. Two well known sites include Nasdaq (formerly Quandl) (<https://commodities-api.com/>,<https://data.nasdaq.com/>) and EOD Historical Data (<https://eodhistoricaldata.com>). There are also a number of public agencies which offer free data APIs such as the U.S. Bureau of Labour Statistics (<https://www.bls.gov/developers/>) and the *FRED*, which stands for the *Federal Reserve Economic Data* (<https://fred.stlouisfed.org/docs/api/fred/>). In the analysis below data APIs from *eodhistoricaldata.com* and the *FRED* are used to import data directly into R.

### Chapter 1: Monthly prices of Crude Oil and Wheat

The monthly price of West Texas Intermediate (WTI) oil can be imported directly with an API from the Federal Reserve of Economic Data (FRED) website. A free API code can be obtained by selecting *FRED Tools -- FRED API* in the menu bar. The FRED API is easy to use because R has a *fredr* package which accommodates an intuitive entry of the search and read parameters.  

```{r fred_api echo off}
fredr_set_key("a63c5424469efa8009045b7d02da6a0f")   

```

There are several typical steps for reading in and preparing the FRED data:

* Filter the data for a start and end date.
* Select and possibly rename the desired columns.
* Reformat the date column to *yearmonth* format (this step is used only to improve the appearance of the data and the graphs).
* Coerce the object into a *tsibble* object.

The code for importing and preparing FRED data for monthly West Texas Intermediate (WTI) crude oil prices is as follows.

```{r crude_mnth}
fred_oil <- fredr(
  series_id = "MCOILWTICO",
  observation_start = as.Date("1986-01-01"),
  observation_end = as.Date("2021-11-01")) %>% 
  dplyr::select(date,value) %>% 
  rename(P_oil = value) %>%
  mutate(month = yearmonth(date)) %>%
  as_tsibble(index=month)
```

The global price of wheat is also imported from FRED and is prepared using the same procedures as crude oil.

```{r wheat}
fred_wht <- fredr(
  series_id = "PWHEAMTUSDM",
  observation_start = as.Date("1990-01-01"),
  observation_end = as.Date("2021-11-01")) %>% 
  dplyr::select(date,value) %>% 
  rename(P_wht = value) %>%
  mutate(month = yearmonth(date)) %>%
  as_tsibble(index=month)
```

The crude oil and wheat data sets can now be joined.
```{r crude_wheat }
wht_oil <- fred_wht %>% inner_join(fred_oil,by="month")
head(wht_oil) 
```

At the end of this chapter optional code is provide for saving the *wht_oil* object as an *RDS* file. 


### Chapter 4 (Part A): Banana Prices

The monthly global price of bananas is read in from an Excel file and converted to a *tsibble* object as follows.

```{r }
bananas <- read_excel(here("data", "bananas.xlsx"),sheet = "bananas") %>%
  mutate(month = yearmonth(Month)) %>%
  dplyr::select(month,Price) %>%
  as_tsibble(index=month) 
head(bananas)
```

### Chapter 4 (Part B): Daily Crude Oil Futures Prices

Daily closing prices for commodities can be accessed using the API offered by *eodhistoricaldata.com*. A free API is available for end-of-day data with the following restrictions: a maximum of 20 API calls per day, and a maximum of one year's worth of daily data. A trial API allows users to retrieve multiple years of rolling crude oil futures prices. Let's use this trial API to download several years of daily crude oil futures prices.

Using the instructions from <https://eodhistoricaldata.com>, the trial API token and the symbol for crude oil futures are as follows:

```{r token}
# trial API for crude oil: http://eodhistoricaldata.com
api.token <- "OeAFFmMliFG5orCUuwAKQ8l4WWFQ67YX"
symbol <- "CL.COMM"
 
```

The following code pulls in daily opening, high, low and closing prices for WTI crude oil for the period January 1, 2017 to December 31, 2021. 

```{r api_trial}
ticker.link <- paste("http://nonsecure.eodhistoricaldata.com/api/eod/", symbol, "?api_token=", api.token, "&from=2017-01-01&to=2021-12-31&period=d", sep="")
data <- read.csv(url(ticker.link))
head(data)
tail(data)


```

This API call looks rather complicated but it follows a standard set of API rules. The paste function concatenates three bits of code: the API URL, <http://nonsecure.eodhistoricaldata.com/api/eod/>, the API token and the parameters which indicate that daily data (d) is being requested, beginning on January 1, 2017 and ending on December 31, 2021. This string of concatenated characters is then placed into R's *read.csv* function and the data is retrieved.

It is useful to have a csv backup of the daily price data in case the previous API fails. If the backup data is to be used the data has format month-day-year rather than year-month day. This means the code below must be changed accordingly. Let's import this backup data now. 

```{r api_trial_csv}
data_bk <- read.csv(here("data","CL.COMM.csv"))
```

To prepare the crude oil prices for formal analysis, it is first necessary to convert the date column from character format to R date format and then convert the data frame into a *tsibble* object. 

```{r import_trial}
crude_daily <- data %>% 
  mutate(Date = lubridate::ymd(Date)) %>% 
  as_tsibble(index=Date)
head(crude_daily)
 
```

### Chapter 4 (Part C): CPI and Potato Prices

The monthly U.S. CPI and an index of the farm price of potatoes can be accessed using the API of the Federal Reserve of Economic Data (FRED). 

Let's import the U.S. CPI from January 1990 to December 2021.

```{r fred_cpi}
cpi_mnth <- fredr(
     series_id = "CPIAUCSL",
     observation_start = as.Date("1990-01-01"),
     observation_end = as.Date("2021-12-01")
   )
```

The FRED CPI data can be prepared and converted into a tsibble object as follows:

```{r cpi_tbl}
cpi_mnth <- cpi_mnth %>% 
  mutate(month = yearmonth(date)) %>%
  as_tsibble(index=month) %>% 
  dplyr::select(month, value) %>%
  rename(CPI = value)
head(cpi_mnth)

```

Now let's use the FRED API to read in the potato price index from January of 2000 to December of 2021.

```{r ppi_potat}
ppi_potat <- fredr(
  series_id = "WPU01130603",
  observation_start = as.Date("2000-01-01"),
  observation_end = as.Date("2021-12-01"))
```

This potato PPI data can be prepared in the same way that the CPI data was prepared.

```{r ppi_tbl}
ppi_potat <- ppi_potat %>% 
  mutate(month = yearmonth(date)) %>%
  as_tsibble(index=month) %>% 
  dplyr::select(month, value) %>%
  rename(PPI = value)
head(ppi_potat)
```

### Chapter 5: Industrial Production and Eggs 

The empirical applications in Chapter 5 requires five data sets. The first is a measure of monthly U.S. industrial production (electric and gas utilities), the second is the monthly U.S. CPI, the third is a data set which combines the monthly prices of wheat and crude oil and the fourth is the monthly producer price of eggs in the Province of Alberta. The combined wheat- crude oil data and the U.S. CPI were prepared in the previous chapters and so they are excluded from this current round of preparation. The industrial production data comes from the Federal Reserve of Economic Data (FRED). The egg data comes from Agriculture and Agri-Food Canada.

We begin by constructing the industrial production data set. This involves reading in industrial production from FRED, filtering the dates from January of 1990 to December of 2021, reformatting the date column and coercing the data frame to a *tsibble* object.

```{r indust}
industrial <- fredr(
  series_id = "IPUTIL",
  observation_start = as.Date("1990-01-01"),
  observation_end = as.Date("2021-12-01")) %>% 
  dplyr::select(date,value) 
head(industrial)
```

```{r indust2}
industrial <- industrial %>% 
  mutate(month = yearmonth(date)) %>%
  as_tsibble(index=month) %>% 
  dplyr::select(month, value) %>%
  rename(industrial = value)
head(industrial)
  
```
 
Reading and preparing the producer price of jumbo eggs in the province of Alberta is more complex. The data for all provinces can be downloaded from the Statistics Canada website one year at a time. The data for each year is stored in a separate worksheet. The data runs from January of 1985 to December of 2021, which is 37 years. Let's use a loop procedure to read in each workbook and store it as a set of data frames with names *y1*, *y2*, etc.

```{r  egg_loop, message=FALSE}
for (i in 1:37) {

year <- eval(parse(text = "i"))

obj <- read_excel(here("data", "Canadian Monthly Egg Prices.xlsx"),
              sheet = year, skip = 9, na = "..") %>%
  clean_names() %>%
  rename(month = dollars_per_dozen,
         price = alberta) %>%
  dplyr::select("month","price") %>%
  slice(1:12)

assign(paste("y", year, sep=""),obj)
} 
```

In this loop the variable *year*, which takes on values of *y1=1*, *y2=2*, etc as *i* moves through the loop, is used to identify the particular worksheet to read in. The data for a particular year is stored in a vector called *obj*. The last line of code assigns the data which is stored in the *obj* vector to a corresponding *yi* vector (e.g., *y1* for the 1985 worksheet, *y2* for the 1986 worksheet, etc.).

The next step is to vertically stack the 37 years of data into long format. There are eloquent ways to do this but to keep things simple a manual *rbind()* function is used. After the stacking a date vector is created to serve as labels for the respective egg prices.  

```{r  }
price <- rbind(y1,y2,y3,y4,y5,y6,y7,y8,y9,y10,y11,y12,y13,y14,y15,y16,y17,y18,y19,y20,y21,y22,y23,y24,y25,y26,y27,y28,y29,y30,y31,y32,y33,y34,y35,y36,y37)

date <- seq(as.Date("1985/1/1"), by = "month", length.out = 444) 
```

The last step is to merge the price and date vectors and then create a *tsibble* object out of the resulting data frame.

```{r  }
eggs <- tibble(price,date) %>%
  mutate(month = yearmonth(date),
         price = as.numeric(price)) %>%
  as_tsibble(index=month) %>%
  dplyr::select(price,month) 
head(eggs)
```

### Chapter 6 (Part A): Gasoline Price
The first data used in this chapter is the monthly price of unleaded gasoline in Vancouver, Canada.  

```{r gas_vancouver}
vancouver <- read.csv(here("data/ch3", "18100001.csv")) %>%
      filter(GEO==c("Vancouver, British Columbia"),
             Type.of.fuel=="Premium unleaded gasoline at self service filling stations") %>%
           rename(Vancouver = VALUE) %>%
           dplyr::select(REF_DATE,Vancouver)
winnipeg <- read.csv(here("data/ch3", "18100001.csv")) %>%
      filter(GEO==c("Winnipeg, Manitoba"),
             Type.of.fuel=="Premium unleaded gasoline at self service filling stations") %>%
              rename(Winnipeg = VALUE) %>%
                dplyr::select(REF_DATE,Winnipeg)

gas_city <- vancouver %>% inner_join(winnipeg, by="REF_DATE")  %>%
  mutate(month = ym(REF_DATE),
    month = yearmonth(month),
    premium = Vancouver-Winnipeg) %>%
      dplyr::select(month,Vancouver,Winnipeg,premium) %>%
  as_tsibble(index=month) %>%
  filter_index("2000-01" ~ .)
```

### Chapter 6 (Part B): Electricity - Natural Gas Margin
The second data used in this chapter consists of a calculated monthly "spark spread", which is the wholesale price of electricity minus the wholesale price of natural gas multiplied by a conversion factor. The spark spread can be viewed as the gross margin earned by plants which produce electricity from natural gas. The electricity price is the monthly average of the Intercontinental Exchange (ICE) daily wholesale spot prices of "Mid C Peak" electricity (\$/MWh). The Mid C Peak price is the price for electricity delivered to the Mid Columbia Trading Hub in the U.S. Pacific Northwest. This data set runs from June of 2001 to December of 2021.

The Federal Reserve of Economic Data (FRED) has data on the Henry Hub natural gas spot price (dollars per million BTU) for the June of 2001 to December of 2021 period. Henry Hub refers to the price established at a major natural gas trading hub near Erath, Louisiana. The price of natural gas at Malin (in Northern California) is a more appropriate price but daily data at this location is available for the years 2015, 2016 and 2017 only. We proceed by calculating the daily percent price difference for Malin and Henry Hub natural gas, and using this calculated percentage to adjust the monthly Henry Hub price to more accurately reflect the prevailing price in the Pacific Northwest. 

The daily gas prices at Malin and Henry Hub are in separate Excel files for the years 2015, 2016 and 2017. We begin by importing these files, filtering the Malin and Henry Hub prices, joining the filtered data and then calculating the log of the Henry Hub price minus the log of the Malin price.

```{r gas_daily}
d2015_gas <- read_excel(here("data/electricity", "ice_natgas-2015final.xls"),
                    sheet = "2015", na = "..") %>% 
  clean_names()

d2016_gas <- read_excel(here("data/electricity", "ice_natgas-2016final.xls"),
                    sheet = "2016", na = "..") %>% 
  clean_names()

d2017_gas <- read_excel(here("data/electricity", "ice_natgas-2017final.xlsx"),
                    sheet = "2017", na = "..") %>% 
  clean_names()

gas_mailin <- rbind(d2015_gas,d2016_gas,d2017_gas) %>%
  filter(price_hub == "Malin") %>% 
  rename(malin_day = wtd_avg_price_mm_btu) %>%
  dplyr::select(trade_date,malin_day)

gas_henry <- rbind(d2015_gas,d2016_gas,d2017_gas) %>%
  filter(price_hub == "Henry") %>% 
  rename(henry_day = wtd_avg_price_mm_btu) %>%
  dplyr::select(trade_date,henry_day)

compare_daily <- gas_mailin %>% inner_join(gas_henry,by="trade_date")
compare_daily <- compare_daily %>% mutate(discount = log(henry_day)-log(malin_day))
discount <- mean(compare_daily$discount)
discount
```

The next step is to import the "Mid C Hub" electricity prices. These prices are in a dedicated *Mid C Hub* file for the years 2001 to 2013, and must be filtered out of individual year files for 2014 to 2021. After importing and filtering, the daily prices can be stacked to create a complete data set from June of 2001 to December of 2021.

```{r import_midc}
# import data from "Mid-C Hub" file
d2001_2013 <- read_excel(here("data/electricity", "MID-C Hub.xls"),
                    sheet = "Sheet1", na = "..") %>% 
  clean_names()

# import individual year files, stack them and filter out "Mid-C Hub"
d2014 <- read_excel(here("data/electricity", "ice_electric-2014final.xls"),
                    sheet = "2014", na = "..") %>% 
  clean_names()

d2015 <- read_excel(here("data/electricity", "ice_electric-2015final.xls"),
                    sheet = "2015", na = "..") %>% 
  clean_names()

d2016 <- read_excel(here("data/electricity", "ice_electric-2016final.xls"),
                    sheet = "2016", na = "..") %>% 
  clean_names()

d2017 <- read_excel(here("data/electricity", "ice_electric-2017final.xlsx"),
                    sheet = "2017", na = "..") %>% 
  clean_names()

d2018 <- read_excel(here("data/electricity", "ice_electric-2018final.xlsx"),
                    sheet = "2018", na = "..") %>% 
  clean_names()

d2019 <- read_excel(here("data/electricity", "ice_electric-2019final.xlsx"),
                    sheet = "2019", na = "..") %>% 
  clean_names()

d2020 <- read_excel(here("data/electricity", "ice_electric-2020final.xlsx"),
                    sheet = "2020", na = "..") %>% 
  clean_names()

d2021 <- read_excel(here("data/electricity", "ice_electric-2021final.xlsx"),
                    sheet = "2021", na = "..") %>% 
  clean_names()

elec_daily_new <- rbind(d2014,d2015,d2016,d2017,d2018,d2019,d2020,d2021) %>%
  filter(price_hub == "Mid C Peak")

# stock two sets of daily prices and rename the price variable
elec_daily <- rbind(elec_daily_new,d2001_2013) %>%
  rename(day_price = wtd_avg_price_m_wh) %>%
  dplyr::select(trade_date,day_price)
```

The next step is to aggregate the daily electricity price data into monthly averages. There are three months of missing data and so the *fill_gaps()* function is used to impute these missing values. After this adjustment, the monthly averages are seasonally adjusted.

```{r elect_sa}
elect_month <- elec_daily %>%
  mutate(month = yearmonth(trade_date)) %>%
  group_by(month) %>% 
  summarise(month_price = mean(day_price)) %>%
  as_tsibble(index=month) %>%
  filter_index("2001-06" ~ .)

electricity <- tsibble::fill_gaps(elect_month)

x11_dcmp <- electricity %>%
  model(x11 = X_13ARIMA_SEATS(month_price ~ x11())) %>%
  components() 

electricity_sa <- electricity %>% mutate(
  elect_sa = x11_dcmp$trend+x11_dcmp$irregular) %>%
  dplyr::select(month,elect_sa) 
```

We now import the Henry Hub monthly natural gas price from FRED, and do the seasonal adjustment.

```{r elec}
fredr_set_key("a63c5424469efa8009045b7d02da6a0f")
 
gas<- fredr(
  series_id = "MHHNGSP",
  observation_start = as.Date("1997-01-01"),
  observation_end = as.Date("2022-10-01")) %>% 
  rename(gas = value) %>%
  mutate(month = yearmonth(date)) %>%
  dplyr::select(month,gas) %>%
  as_tsibble(index=month) 

x11_dcmp <- gas %>%
  model(x11 = X_13ARIMA_SEATS(gas ~ x11())) %>%
  components()

gas_sa <- gas %>% mutate(
  gas_sa = x11_dcmp$trend+x11_dcmp$irregular) %>%
  dplyr::select(month,gas_sa)

```

The last step is to join the electricity and gas seasonally adjusted price and then calculate the spark spread. The spark spread is calculated as:

*Spark spread ($/MWh) = power price ($/MWh) â€“ [natural gas price ($/mmBtu) heat rate (mmBtu/MWh)] *. The Energy Information Administration (EIA) recommends a heat rate coefficient equal to 7,000 Btu/kilowatthour (kWh), which is equivalent to 7 mmBTU/MWh. When applying this formula we must multiply the natural gas price by *1 - discount* where *discount* was calculated above.

```{r spark_spread}
power <- gas_sa %>% 
  inner_join(electricity_sa, by="month") 

power <- power %>%
  mutate(SprkSprd = elect_sa - (gas_sa*(1-discount)*7))


```

### Chapter 6 (Part C): Simulated Structural Break Data
Chapter 6 requires simulate trend stationary data with a two separate breaks in the intercept of stochastic process. The following code randomly creates this data and stores the 18 simulated observations as an *RDS* file.

```{r simulate_break}
set.seed(40)

T <- 18
a <- .25
b <- 0.5
c <- 5
del2 <- 6
del3 <- 8
gam <- 2

#unit <- matrix(1,T,1)
#trnd <- seq(1,T)
#V <- as.matrix(cbind(unit,trnd))

poly1 <- a*seq(1,T/3)^2 + b*seq(1, T/3) + c  
poly2 <- poly1 + poly1[T/3] + del2
poly3 <- poly1 + poly2[T/3] + del3
poly <- c(poly1,poly2,poly3)
w <- poly + gam*rnorm(T)
break_simulate <- w
```

### Chapter 7: Fruit, Corn, Energy, Hogs and Exchange Rate

The analysis in this chapter uses the following monthly U.S. data sets:

* Strawberry price. 
* Grape price.
* U.S. dollar - Mexican pesos exchange rate.
* U.S. - Canada dollar exchange rate.
* Price of corn received by farmers
* Temperature anomaly (i.e., deviation from long term average).
* Energy consumption.
* Electricity price index.
* Slaughter hog PPI.

The first three data sets are merged to create a combined fruit data set. The corn price and U.S. - Mexico exchange rate are bundled into the corn data set.  Temperature, energy consumption and the electricity price index are combined into a single data set. The slaughter hog PPI is combined with the price of crude oil (previously imported) and the U.S. - Canada and U.S. - Mexico exchange rates..

**U.S. Strawberry and Grape Prices**

We begin by reading in monthly U.S. prices for strawberries and grapes from FRED. The date range is restricted to January of 2001 to September of 2021. Some of the observations in these two data series have a *NA*, which indicates "not available". The *na.approx()* function replaces each *NA* with a value that is calculated using linear interpolation. This function does not work if the first observation is an *NA*, which is the case for the strawberry price series. To get around this problem observations prior to January of 2001 are inital read in, and these three values are deleted after the *NA* values have been replaced. 

After renaming each price variable the data is coerced into a *tsibble* object.

```{r fruit}
fred_straw <- fredr(
  series_id = "APU0000711415",
  observation_start = as.Date("2000-10-01"),
  observation_end = as.Date("2021-9-01")) %>% 
  mutate(month = yearmonth(date),
         value = na.approx(value)) %>%
  slice(-c(1:3)) %>%
  rename(strawberry =value) %>%
  as_tsibble(index=month) %>%
  dplyr::select(month,strawberry)
  
fred_grape <- fredr(
  series_id = "APU0000711417",
  observation_start = as.Date("2001-01-01"),
  observation_end = as.Date("2021-9-01")) %>% 
  mutate(month = yearmonth(date),
         value = na.approx(value)) %>%
  rename(grape =value) %>%
  as_tsibble(index=month) %>%
  dplyr::select(month,grape)

```

The next step is to import from FRED the the U.S. dollar -- Mexican peso monthly exchange rate. The imported data is prepared in the usual way. 

```{r fruit3}
 ex_mex <-  fredr(
  series_id = "EXMXUS",
  observation_start = as.Date("1980-01-01"),
  observation_end = as.Date("2022-8-01")) %>% 
  rename(ex_mex = value) %>%
  mutate(month = yearmonth(date)) %>%
  dplyr::select(month,ex_mex) %>%
  as_tsibble(index=month)
```
 
The last step is to merge the strawberry price, grape price and exchange rate data.  
```{r fruit4}
fruit <- fred_straw %>% 
  inner_join(fred_grape, by = "month") %>% 
  inner_join(., ex_mex, by = "month") %>%
  mutate(mnth_ind = month(month),
         year = year(month)) 

```

**Gasoline Price (With and Without Seasonal Adjustments) **

The data for the seasonally adjusted and non-adjusted U.S. gasoline price (monthly) is imported from FRED. Along with the import the usual data preparation tips are applied. We will used a capital letter on the variable name to indicate that the series has been seasonally adjusted.

```{r gas1}
 fred_gasSA <- fredr(
  series_id = "CUSR0000SETB01",
  observation_start = as.Date("2000-01-01"),
  observation_end = as.Date("2022-8-01")) %>% 
  mutate(month = yearmonth(date)) %>%
  rename(Gasoline =value) %>%
  as_tsibble(index=month) %>%
  dplyr::select(month,Gasoline) 
 
fred_gas <- fredr(
  series_id = "CUUR0000SETB01",
  observation_start = as.Date("2000-01-01"),
  observation_end = as.Date("2022-8-01")) %>% 
  mutate(month = yearmonth(date)) %>%
  rename(gasoline =value) %>%
  as_tsibble(index=month) %>%
  dplyr::select(month,gasoline) 

 
```

The two gasoline data series can now be merged.

```{r gas2}
gasoline <- fred_gasSA %>% 
  inner_join(fred_gas, by = "month") 
head(gasoline)

```
 
**Price of Corn Received by U.S. Farmers **
The corn price data is imported from an Excel workbook. Preparation consists of  selecting the columns, creating a new date column and then filtering the dates.


```{r corn1}
corn <-  read_excel(here("data/ch3", "corn.xlsx"),sheet = "Feed_Grains_Excel", skip = 1) %>%
  mutate(month_ind = match(Frequency,month.abb),
    month = as.yearmon(paste(Year, month_ind), "%Y %m"),
    month = yearmonth(as.Date(month))) %>%
  rename(corn = Amount) %>%
  dplyr::select(month,month_ind,Year,corn) %>%
  as_tsibble(index = month) %>%
  filter_index("1980 Jan"~.)

```

The next step is to create a new table which is the average annual price. This annual average is used to calculate the monthly deviation from the annual average in the original data frame.

```{r corn2}
year_mean <- as_tibble(corn) %>%
  group_by(Year) %>%
  dplyr::summarize(avg_yr = mean(corn, na.rm = TRUE))

corn <- corn %>% inner_join(year_mean,by="Year") %>%
  mutate(deviate = corn - avg_yr)

```

The last step is to add the U.S. dollar -- Mexican Peso exchange rate to the corn data frame.

```{r corn3}
corn <- corn %>% inner_join(ex_mex, by = "month")

```


**Temperature Anomaly Data**

A temperature anomaly is the average temperture for the current month minus the long term average for that month. The temperature anomaly data resides in 12 worksheets of an Excel workbook, with a worksheet corresponding to each month. For example, the "Jan" worksheet contains the temperature anomaly data for each year from 1895 to 2022. Let's begin by importing the 12 data sets. 

```{r temp_raw}
jan <- read_excel(here("data", "temperature.xlsx"),
                  sheet = "Jan", skip = 1, na = "..") %>%
  mutate(Month="01")

feb <- read_excel(here("data", "temperature.xlsx"),
                  sheet = "Feb", skip = 1, na = "..") %>%
  mutate(Month="02")

mar <- read_excel(here("data", "temperature.xlsx"),
                  sheet = "Mar", skip = 1, na = "..") %>%
  mutate(Month="03")

apr <- read_excel(here("data", "temperature.xlsx"),
                  sheet = "Apr", skip = 1, na = "..") %>%
  mutate(Month="04")

may <- read_excel(here("data", "temperature.xlsx"),
                  sheet = "May", skip = 1, na = "..") %>%
  mutate(Month="05")

jun <- read_excel(here("data", "temperature.xlsx"),
                  sheet = "Jun", skip = 1, na = "..") %>%
  mutate(Month="06")

jul <- read_excel(here("data", "temperature.xlsx"),
                  sheet = "Jul", skip = 1, na = "..") %>%
  mutate(Month="07")

aug <- read_excel(here("data", "temperature.xlsx"),
                  sheet = "Aug", skip = 1, na = "..") %>%
  mutate(Month="08")

sep <- read_excel(here("data", "temperature.xlsx"),
                  sheet = "Sep", skip = 1, na = "..") %>%
  mutate(Month="09")

oct <- read_excel(here("data", "temperature.xlsx"),
                  sheet = "Oct", skip = 1, na = "..") %>%
  mutate(Month="10")

nov <- read_excel(here("data", "temperature.xlsx"),
                  sheet = "Nov", skip = 1, na = "..") %>%
  mutate(Month="11")

dec <- read_excel(here("data", "temperature.xlsx"),
                  sheet = "Dec", skip = 1, na = "..") %>%
  mutate(Month="12")

```

The 12 data sets can now be combined using *year* as the common variable. The *arrange()* function is used to convert the wide data into the desired long data format  (i.e., a format with the monthly temperature anomaly data stacked rather than placed side-by-side). 

```{r temp_bind}
temp <- rbind(jan,feb,mar,apr,may,jun,jul,aug,sep,oct,nov,dec) %>%
  rename(Year=Date) %>%
  mutate(Year=as.numeric(Year),
         Month=as.numeric(Month),
         Date=make_date(Year,Month),
         month=yearmonth(Date)
         )
temp2 <-arrange(temp, Year, Month) %>%
  filter(Year>=1986) %>%
  slice(-n())
head(temp2)
```

**Merge with Energy Consumption Data and Electricity Price Data**

Energy consumption data, which runs from January 1973 to April 2022, consists of monthly U.S. residential energy use. Let's read in this data from a csv file and then convert the imported data into a *tsibble* object.

```{r energy1}
ener_dat <- read.csv(here("data","energy.csv")) %>%
  mutate(month = yearmonth(date)) %>%
  as_tsibble(index = month) %>%
  dplyr::select(month,energy)
head(ener_dat)

```

The electricity price index is imported from FRED.
The seasonally adjusted version of this data was imported above. 

```{r elec1}
fredr_set_key("a63c5424469efa8009045b7d02da6a0f")
 
elect<- fredr(
  series_id = "CUUR0000SEHF01",
  observation_start = as.Date("1990-01-01"),
  observation_end = as.Date("2022-08-01")) %>% 
  rename(elect_nom = value) %>%
  mutate(month = yearmonth(date)) %>%
  dplyr::select(month,elect_nom) 

```

We will work with the real price of electricity and so let's add the U.S. all-items CPI to the previous electricity data frame and then calculate a real price of electricity.

```{r elec1b}
elect <- elect %>% 
  inner_join(cpi_mnth,by="month") %>%
  mutate(electricity = 100*elect_nom/CPI)

```

The energy consumption, electricity price and temperature anomaly data can now be merged.

```{r energy2}
energy_temp <- ener_dat %>% 
  inner_join(temp2,by="month")  %>%
  inner_join(., elect, by="month") %>%
  dplyr::select(month,energy,electricity,ClimDiv) %>%
  rename(temperature=ClimDiv) %>%
  slice(-(1:2))
head(energy_temp)

```

**Slaughter Hog and Exchange Rate Data**

The data set for the last analysis in Chapter 6 consists of the slaughter hog producer price index, the price of WTI crude oil (previously imported), the U.S. - Canada exchange rate. These data series can be imported from the FRED database. 

```{r hogs1}

hogs <-  fredr(
  series_id = "WPU0132",
  observation_start = as.Date("1986-01-01"),
  observation_end = as.Date("2021-12-01")) %>% 
  rename(hogs = value) %>%
  mutate(month = yearmonth(date)) %>%
  dplyr::select(month,hogs) %>%
  as_tsibble(index=month)

 ex_can <-  fredr(
  series_id = "EXCAUS",
  observation_start = as.Date("1986-01-01"),
  observation_end = as.Date("2021-12-01")) %>% 
  rename(ex_can = value) %>%
  mutate(month = yearmonth(date)) %>%
  dplyr::select(month,ex_can) %>%
  as_tsibble(index=month)

```

This pair of data sets should be merged with the previously imported price of WTI crude oil (fred_oil) and the U.S. - Mexican exchange rate (ex_mex). The resulting data frame should then be coerced into a *tsibble* object.  

```{r hogs2}
hogs <- hogs %>% 
  inner_join(ex_can, by = "month") %>%
  inner_join(fred_oil, by="month") %>%
   as_tsibble(index=month) %>%
  rename(oil = P_oil) %>%
  dplyr::select(-date)

```

## Chapter 7: Simulated ARIMA Data

Chapter 7 will make uses of quarterly ARIMA(1,1,1) simulated data. The simulation creates the raw ARIMA data series using the *arima.sim()* function and merges this data with a date sequence.

```{r simulate1}
set.seed = 10
Z <- arima.sim(list(order = c(1,1,1), ar = 0.8, ma = 0.2), n = 80) 
seq <- seq(as.Date("2002/1/1"), as.Date("2022/1/1"), by = "quarter") 
data_sim <- dplyr::bind_cols(tibble(seq),tibble(Z)) %>%
  mutate(quarter = yearquarter(seq)) %>%
  as_tsibble(index=quarter) %>%
  dplyr::select(quarter,Z) 
   
```

The next step is to generate an exogenous *x* variable which is correlated with the ARIMA series. As well, a second ARIMA series is created by adding seasonal components to the first ARIMA series.

```{r simulate2}
data_sim <- data_sim %>% 
  mutate(rnd1 = 2*rnorm(81),
         rnd2 = rnorm(81),
         x = 20 + 0.8*Z + rnd1,
    qtr = quarter(quarter),
    s1 = ifelse(qtr==1,-5 +rnd2,0),
    s2 = ifelse(qtr==2,-2 +rnd2,0),
    s3 = ifelse(qtr==3, 2 +rnd2,0),
    s4 = ifelse(qtr==3, 2 +rnd2,0),
    z = Z + s1 + s2 + s3 + s4) %>%
  dplyr::select(quarter,Z,z,x)
head(data_sim)   
```


### Chapter 8: Woodchips and Heating Oil

The wood-energy data set consists of the prices of wood chips, plywood, heating oil and crude oil. Similar to the import procedure used for potato prices in Chapter 6, the four prices in this data set are imported from the FRED website via an api (the data for crude oil was previously imported but it will be repeated here because the start data is earlier). Preparing this data involves filtering the data to begin in January of 1986 and ending in April of 2022. As well, a descriptive name of each time series is used to replace "value" as the series name.

```{r import}
# import crude oil prices from FRED
crude_day <- fredr(
  series_id = "DCOILWTICO",
  observation_start = as.Date("1986-01-01"),
  observation_end = as.Date("2022-04-01")) %>% 
  dplyr::select(date,value) %>% 
  rename(crude_day = value)

# import chip prices from FRED
chips <- fredr(
  series_id = "PCU3211133211135",
  observation_start = as.Date("1986-01-01"),
  observation_end = as.Date("2022-04-01")) %>% 
  dplyr::select(date,value) %>% 
  rename(chips_lev = value,
         month = date)

# import plywood prices from FRED
plywood <- fredr(
  series_id = "WPU083",
  observation_start = as.Date("1986-01-01"),
  observation_end = as.Date("2022-04-01")) %>% 
  dplyr::select(date,value) %>% 
  rename(plywood_lev = value,
         month = date)

# import heating oil prices from FRED
heat <- fredr(
  series_id = "WPU05730201",
  observation_start = as.Date("1986-01-01"),
  observation_end = as.Date("2022-04-01")) %>% 
  dplyr::select(date,value) %>% 
  rename(heat_lev = value,
         month = date)

```

An additional preparation step is required because the fourth data series (crude oil prices) arrives in daily format rather than monthly format. The daily data is aggregated into monthly average prices for crude oil as follows.

```{r oil_month}
crude_day <- crude_day[complete.cases(crude_day),] 
crude <- crude_day %>%
  mutate(month = yearmonth(date)) %>%
  group_by(month) %>% 
  summarise(crude_lev = mean(crude_day))
   
```

The four imported prices for wood chips, plywood, heating oil and crude oil can now be merged and the resulting data frame can then be coerced into a *tsibble* time series object.

```{r join_all}
wood <- chips %>% inner_join(plywood, by = "month") %>%
  inner_join(heat, by = "month") %>%
  inner_join(crude, by = "month") %>%
  as_tsibble(index=month) 

```

This wood - energy data set will be used in Chapter 7 to estimate a vector autoregression (VAR) model with price premium variables. The wood chip price premium is the log of the difference between the price of wood chips and the price of plywood (recall that the log of the price difference is approximately equal to the percent difference). The heating oil price premium is the log of the difference between the price of heating oil and the price of crude oil. Let's add these two price premium variables to our data set and then deselect the original price variables.

```{r premium}
wood <- wood %>% 
  mutate(premE=log(heat_lev-crude_lev),
         premW=log(plywood_lev-chips_lev)) %>%
  dplyr::select(month,premE,premW)

```

The price premium data requires a month variable for the purpose of creating seasonality variables. The month variable is added as follows.

```{r month}
wood <- wood %>% 
  mutate(period = as.numeric(format(month, "%m")))

```

The final step is to merge the price premium data sets with the temperature anomaly data which was created in Chapter 6.  

```{r final}
wood <- wood %>% 
  mutate(temp=temp2$ClimDiv) %>%
  mutate(L.temp=lag(temp),
         L2.temp=lag(temp,2))
head(wood)
```

### Chapter 9: Vegetable Oils and Biofuels

This data set consists of the monthly prices of soybeans, rapeseed, palm oil. These are prices are extracted from a large World Bank data set. The monthly data runs from January 1960 to the current month. Importing the World Bank data into R is complicated for several reasons: (1) the column names reside in row 7; (2) the column names are in all capital letters; (3) there is no name for the date column; and (4) missing data is identified with ".." rather than "na".

The *clean_names()* function will convert capital letters to small letters in the column names, and will also assign "x1" as the name for the date column. Assigning *skip = 6* and *na = ".."* ensures that the first six rows are skipped when reading the data, and all occurrences of ".." are converted to "na". Let's import the data now.

Using these procedures the data can be imported as follows.

```{r veg_import}
cmo <- read_excel(here("data/ch3", "CMO-Historical-Data-Monthly.xlsx"),
                  sheet = "Monthly Prices", skip = 6, na = "..") %>%
  clean_names()

```

The date format in the imported data is a little unusual (e.g., "1960M01"). Nevertheless, the *ym()* function from R's lubridate package is able to manage this conversion (note that the "x1" date column is renamed "month"). A filter function is used to select the rows which correspond to January of 2003 to December of 2021.

```{r veg_filter}
 
cmo <- cmo %>%
  rename(month = x1) %>%
  dplyr::select(month, palm_oil, soybean_oil, rapeseed_oil) %>%
  mutate(month = ym(month)) %>%
  filter(month >= "2003-01-01" & month <= "2020-12-01")
  head(cmo)

```

The final step is to reformat the month column to make it more readable. The reformatted month column can now serve as the index when coercing the imported data frame into a *tsibble* time series object.

```{r veggie_oil}
  
cmo <- cmo %>% mutate(month = yearmonth(month)) %>% 
  as_tsibble(index = month) %>%
  dplyr::select(month, everything()) 
  head(cmo)

```

The next data set to be created requires combining the weekly price of soybean oil and biodiesel, regular diesel and crude oil. The first pair of prices will be imported from an csv file and the second pair from individual Excel files. An inner join is used to merge these three data sets. The final data set runs from week 2 of 1994 to week 2 of 2022. 

When reading in the soybean oil and biodiesel data, the first two rows are skipped and the first two columns, which contain the desired prices, are selected. The names of the two columns are then changed. Finally, the date which is imported in character format is converted to an R date format using the *mdy()* function. The procedure for reading in the price of diesel from one of the Excel files and the price of crude oil from the second Excel file is similar.  

```{r biodiesel}
iowa_data <- read_csv(here("data/ch3", "hist_bio_gm.csv"), skip = 2) %>%
  dplyr::select(1:3) %>%
  dplyr::rename(date = 1, biodiesel = 2, soyoil = 3) %>%
  mutate(date = mdy(date)) # convert character to date format
head(iowa_data)

diesel_data <- readxl::read_excel(here("data/ch3","Diesel Fuel Prices-EIA.xls"), sheet = "Data 1", skip = 2)  %>% 
  dplyr::select(1,2) %>%
  dplyr::rename(date = 1, diesel = 2) %>%
  mutate(date = ymd(date))
head(diesel_data)

crude_data <- readxl::read_excel(here("data/ch3","Crude Oil Prices-EIA.xls"), sheet = "Data 1", skip = 2)  %>% 
  dplyr::select(1,2) %>%
  dplyr::rename(date = 1, crude = 2) %>%
  mutate(date = ymd(date))
head(crude_data)

```

The next step is to recognize that the dates for the soybean and biodiesel data do not perfectly match with the dates for the diesel and crude oil data (e.g., prices within the first set are measured on a Friday, and prices for the second set are measured on a Monday). It is useful to use the *yearweek()* function from the *tsibble* package to assign a common year-week ID for both sets of data. After creating the new *week* variable, the original *date* variable is deselected so that it is no longer part of the tibble.


```{r biodiesel2}
iowa_data <- iowa_data %>% 
  mutate(week = yearweek(date)) %>% 
  as_tibble(index=week) %>% 
  dplyr::select(-date)

diesel_data <- diesel_data %>% 
  mutate(week = yearweek(date)) %>% 
  as_tibble(index=week) %>% 
  dplyr::select(-date)

crude_data <- crude_data %>% 
  mutate(week = yearweek(date)) %>% 
  as_tibble(index=week) %>% 
  dplyr::select(-date)

```

With the dates aligned across the three data sets, it is possible to merge the two data sets. The joins are done in pairs and so in the first join the diesel and crude oil prices are merged, and in the second join the soybean oil and biodiesel prices are merged with the diesel and crude oil prices. Note that the *inner_join()* function from the *dplyr* package includes all rows which are present in both sets of data. After the join the *date* column is moved to the right of the last column. The merged data is then converted into a tibble and the *distinct()* function from the *dplyr* package is used to eliminate duplicate rows. After moving the *week* column to the left of the first column, the tibble is converted into a time series *tsibble*. 

```{r biodiesel3}
diesel_crude <- inner_join(diesel_data, crude_data,by="week")
data <- inner_join(iowa_data,diesel_crude,by="week") %>% 
  relocate(week) %>%
  as_tibble(index=week) %>% 
  distinct() %>% 
  as_tsibble(index = week)
head(data)

```

When working with weekly time series data it is common for the data for some weeks to be missing. With the data residing in tsibble object, the *check_gaps()* function can be used to identify the gaps. Note that when working with *tsibble* objects the *forecast* package must be loaded for the *check_gaps()* function to work:

```{r biodiesel4}
data_gaps <- data %>% 
  count_gaps(.full = TRUE)
data_gaps

```

We can see that data is missing for three consecutive weeks in 2013, one week in 2016 and one week in 2017. Data can be imputed for these missing weeks using the *fill_gaps()* function:

```{r biodiesel5}
biofuel <- data %>% 
  fill_gaps(.full = TRUE) %>%
  mutate(
    biodiesel = na.interp(biodiesel),
    soyoil = na.interp(soyoil), 
    diesel = na.interp(diesel), 
    crude = na.interp(crude))  
 head(biofuel) 

```

### Chapter 10 (Part A): Canada - U.S. Crude Oil

This first data set for Chapter 10 consists of the monthly prices of West Texas Intermediate (wti) and Western Canada Select (wcs) crude oil plus the U.S. dollar index. 

The pair of oil prices are read in from an Excel file. The usual preparation procedure is performed, which consists of converting the date to a month format and then using this converted date as the index for a *tsibble* time series object. The data is filtered to begin in January of 2005, which is the first month the wcs data is available. 

```{r read_oil, warning=FALSE}
oil <- read_excel(here("data/ch3", "wti_wcs_price.xlsx"),
      sheet = "amCharts", col_types = c("date", "skip", "numeric", "numeric","numeric")) %>%
  mutate(month = yearmonth(date)) %>%
  as_tsibble(index=month) %>%
  dplyr::select(month,wti,wcs) %>%
  filter_index("2005 Jan" ~.)
head(oil) 
```

To complete the first data set the U.S. dollar index is imported through use of a FRED api:

```{r dollar}
# import crude oil prices from FRED
dollar <- fredr(
  series_id = "TWEXBGSMTH",
  observation_start = as.Date("2006-01-01"),
  observation_end = as.Date("2022-08-01")) %>% 
  rename(dollar = value) %>%
  mutate(month = yearmonth(date)) %>%
  dplyr::select(month,dollar) 
head(dollar)
```

The dollar index and the oil price data can now be merged.

```{r oil_final}
oil <- oil %>% inner_join(dollar,by="month")
head(oil)

```

### Chapter 10 (Part B): Diesel Fuel, Rainfall and Vegetable CPI
 
The second data set is created by merging three individual monthly U.S. data sets: (1) fruit and vegetable CPI (F&V CPI); (2) price of diesel fuel; (3) drought variables for six agricultural counties in California. The first two data sets come from FRED and the last one is imported from a series of worksheets in an Excel workbook. We begin by reading in the first two data sets and then aggregating from weekly to monthly data for the diesel data set. 

```{r veggie_import}
veggie_df <- fredr(
  series_id = "CUSR0000SAF113",
  observation_start = as.Date("1996-04-01"),
  observation_end = as.Date("2022-12-01")) %>% 
  rename(veggies = value) %>%
  mutate(month=yearmonth(date)) %>%
  dplyr::select(month,veggies)   

diesel_wk <- fredr(
  series_id = "GASDESW",
  observation_start = as.Date("1996-04-01"),
  observation_end = as.Date("2022-12-01")) %>% 
  dplyr::select(date,value) %>% 
  rename(diesel_wk = value) %>%
  as_tsibble(index=date)

diesel_df <-diesel_wk %>%
  index_by(month = ~ yearmonth(.)) %>%  
  summarise(
    diesel = log(mean(diesel_wk))) 

```

The drought data set consists of separate Excel worksheets for six of the leading agricultural counties in California: Fresno, Monterey, Kern, Ventura, San Diego and Imperial. Each worksheet contains several variables for measuring the percent of the land in the county which is experiencing drought for that particular week. Let's import the five worksheets now. 

```{r drought_import}
drought_fresno <- read_excel(here("data/ch3","Data Tables  U.S. Drought Monitor.xlsx"),sheet="Fresno",skip = 1) %>%
  dplyr::select(Week,"D2-D4","D3-D4","D4") %>%
  rename(d2_4=2,
         d3_4=3,
         d4=4) %>%
  mutate(Week = as.Date(Week)) %>%
  as_tsibble(index=Week)

drought_monterey <- read_excel(here("data/ch3","Data Tables  U.S. Drought Monitor.xlsx"),sheet="Monterey",skip = 1) %>%
  dplyr::select(Week,"D2-D4","D3-D4","D4") %>%
  rename(d2_4=2,
         d3_4=3,
         d4=4) %>%
  mutate(Week = as.Date(Week)) %>%
  as_tsibble(index=Week)

drought_kern <- read_excel(here("data/ch3","Data Tables  U.S. Drought Monitor.xlsx"),sheet="Kern",skip = 1) %>%
  dplyr::select(Week,"D2-D4","D3-D4","D4") %>%
  rename(d2_4=2,
         d3_4=3,
         d4=4) %>%
  mutate(Week = as.Date(Week)) %>%
  as_tsibble(index=Week)

drought_ventura <- read_excel(here("data/ch3","Data Tables  U.S. Drought Monitor.xlsx"),sheet="Ventura",skip = 1) %>%
  dplyr::select(Week,"D2-D4","D3-D4","D4") %>%
  rename(d2_4=2,
         d3_4=3,
         d4=4) %>%
  mutate(Week = as.Date(Week)) %>%
  as_tsibble(index=Week)

drought_sandiego <- read_excel(here("data/ch3","Data Tables  U.S. Drought Monitor.xlsx"),sheet="San Diego",skip = 1) %>%
  dplyr::select(Week,"D2-D4","D3-D4","D4") %>%
  rename(d2_4=2,
         d3_4=3,
         d4=4) %>%
  mutate(Week = as.Date(Week)) %>%
  as_tsibble(index=Week)

drought_imperial <- read_excel(here("data/ch3","Data Tables  U.S. Drought Monitor.xlsx"),sheet="Imperial",skip = 1) %>%
  dplyr::select(Week,"D2-D4","D3-D4","D4") %>%
  rename(d2_4=2,
         d3_4=3,
         d4=4) %>%
  mutate(Week = as.Date(Week)) %>%
  as_tsibble(index=Week)
 
```

After creating six data frames from the imported worksheets a new data frame is created which is the average weekly value across the six counties for each drought variable. The weekly averages are then aggregated into a monthly average for each drought variable.

```{r drought_avg}
# https://stackoverflow.com/questions/31465415/combine-multiple-data-frames-and-calculate-average
library(data.table)
drought_wk <- rbindlist(list(drought_fresno,drought_monterey,drought_kern,drought_ventura,drought_sandiego,drought_imperial))[,lapply(.SD,mean), list(Week)] %>%
   as_tsibble(index=Week)

drought_df <-drought_wk %>%
  index_by(month = ~ yearmonth(.)) %>%  
  summarise(
    d2_4 = mean(d2_4),
    d3_4 = mean(d3_4),
    d4 = mean(d4)) 

```

The five data frames (F&V cpi, meat cpi, diesel price, regular cpi and drought) can now be merged into a single data frame named *veggie*.

```{r rain, warning=FALSE}
list_df = list(veggie_df,diesel_df,drought_df)
veggie <- list_df %>% reduce(inner_join, by='month') %>% 
  mutate(mnth = month(month)) %>%
  as_tsibble(index=month)
head(veggie)
```

 
### Chapter 11: Data to be Determined

The data set for this chapter has not yet been finalized. 

## Saved RDS files {#sec:rds}

This section consists of the code for saving the data set for each chapter into a *.RDS* file. At the beginning of the empirical application in each chapter, the corresponding *.RDS* file is imported in order to make the prepared data available. THe lines of code are currently commented out. The code should be made live and the save initiated if a change is made to one or more of the data sets. 

```{r save}
 
# saveRDS(wht_oil, here("data/ch3", "wht_oil.RDS"))
# saveRDS(crude_daily, here("data/ch3", "crude_daily.RDS")) 
# saveRDS(cpi_mnth, here("data/ch3", "cpi_mnth.RDS"))
# saveRDS(bananas, here("data/ch3", "bananas.RDS"))
# saveRDS(ppi_potat, here("data/ch3", "ppi_potat.RDS"))
# saveRDS(industrial, here("data/ch3", "industrial.RDS"))
# saveRDS(eggs, here("data/ch3", "eggs.RDS"))
# saveRDS(gas_city, here("data/ch3", "gas_city.RDS"))
# saveRDS(break_simulate, here("data/ch3", "break_simulate.RDS"))
# saveRDS(power, here("data/ch3", "power.RDS"))
# saveRDS(fruit, here("data/ch3", "fruit.RDS"))
# saveRDS(gasoline, here("data/ch3", "gasoline.RDS"))
# saveRDS(corn, here("data/ch3", "corn.RDS"))
# saveRDS(cmo, here("data/ch3", "vegoils.RDS")) 
# saveRDS(energy_temp, here("data/ch3", "energy_temp.RDS"))
# saveRDS(hogs, here("data/ch3", "hogs.RDS"))
# saveRDS(data_sim, here("data/ch3", "data_sim.RDS"))
# saveRDS(wood, here("data/ch3", "wood.RDS"))
# saveRDS(biofuel, here("data/ch3", "biofuel.RDS"))
# saveRDS(oil, here("data/ch3", "oil.RDS"))
# saveRDS(veggie, here("data/ch3", "veggie.RDS"))
```
 
### Extra: Lumber Data

This last data set on lumber prices is currently not being used in any of the empirical analysis. It consists of the monthly prices of four types of U.S. lumber, one type of Swedish lumber and the U.S. - Sweden exchange rate. The data runs from January of 1997 to December of 2013. The U.S. prices are in a csv file, the Swedish price is in an Excel file and the exchange rate is in a csv file.

The dates in the U.S. data are into two separate columns (i.e., month with an abbreviated label in one column and year in a second column). The *match()* function matches the month abbreviation to a standard full month label. The *paste()* function is then used to paste the month and year labels together to create a new column which contains a month-year combo label. The *ym()* function can be applied to this new column to create the R date. The original month and year columns can then be deselected.

```{r lumber}
lumbUSA <- read_csv(here("data/ch3", "Lumber Assortment USA.csv"))
 lumbUSA <- lumbUSA %>% 
   mutate(date = ym(paste(year, match(month,month.abb), sep="-"))) %>%
   select(!c(year,month))

```

Preparing the Swedish lumber price data is relatively complex because the Excel file is organized to be used with an Excel search macro. It is best to assign no column names and to assume that all incoming data is in text format. If this latter option is not used then R will generate warnings due to a failure of the auto-detect function. 

After the Swedish data is read in, the month and year columns, and the column which contains the price of the selected Swedish wood product are selected. As well, the data is sliced so that only those rows which correspond to the U.S. data are retained in the tibble. The preparing the Swedish table concludes by converting the prices from text to numeric and converting the month and year columns into a single R date. Also, the original month and year columns are deselected, similar to what was done for the U.S. lumber data. 

```{r lumber2, message = FALSE}
lumbSWD <- readxl::read_excel(here("data/ch3","PriceOutputTable.xls"), sheet = "Data", col_names = FALSE, col_types = "text") %>%
  dplyr::select(4,5,16) %>%
  rename(year = 1, month = 2, sweden = 3) %>%
  slice(462:749) %>%
  mutate(sweden=as.numeric(sweden)) %>%
  mutate(date = ym(paste(year, match(month,month.abb), sep="-"))) %>%
  dplyr::select(!c(year,month))
head(lumbSWD)
```

Reading in the exchange rate data is straight forward because there are only two columns and the date column has a standard format:

```{r lumber3}
exchange <- read_csv(here("data/ch3", "EXSDUS.csv")) %>%
  mutate(date = mdy(DATE)) %>%
  select(!DATE) 
```

We can now merge the prepared U.S. and Swedish data, and then merge the resulting tibble with the exchange rate data. The last step is to convert the resulting tibble to a time series tsibble object.

```{r lumber4}
lumber_both <- merge(lumbUSA,lumbSWD,by="date")
lumber <- merge(lumber_both,exchange,by="date") %>%
as_tsibble(index = date)
head(lumber)

```

## Conclusions

In this chapter a variety of methods were used to import and prepare data. The easiest preparation is for the data imported by API from the FRED website. This is because the data is standardized and so the same preparation steps can be used for each data set. Importing data from Excel files and csv files are similar regarding preparation steps. Excel has the advantage that a "source" worksheet can be used to describe where and when the data was accessed. It is important to identify when the data was accessed from a website so that different versions of the data can be effectively managed.

When accessing data from a website it is tempting to do some of the initial preparation steps in Excel. For example, it may be tempting to delete a blank row between the header and the first row of data, or perhaps merge two data sets before reading the data into R. Resist this temptation because it moves the analysis further from reproducibility. The goal is to make it as easy as possible for another researcher to be able to reproduce your results. It is for this reason why it is preferable to import data using API rather than downloading the data to a csv or Excel file and then reading the data into R.

{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```
